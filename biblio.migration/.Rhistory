sum(is.na(cluster_id)) > 0,
by=.(source_id, transition_year)][
V1 == FALSE,
.(source_id, transition_year)
]
missing_transitions_partial <- editors_partial[
transition_year <= 2022,
sum(is.na(cluster_id)) > 0,
by=.(source_id, transition_year)][
V1 == TRUE,
.(source_id, transition_year)
]
# Adapted from:
# https://stackoverflow.com/questions/51150522/subset-in-list-of-tuples-multiple-columns-with-in-clause
editors_partial[
,
'still_missing_partial' := apply(editors_partial, 1, match_tuple, missing_transitions_partial, 'source_id', 'transition_year')
]
editors_partial[still_missing_partial == TRUE]
missing_editors <- rbind(
editors[still_missing == TRUE],
editors_partial[still_missing_partial == TRUE],
fill=TRUE
)[is.na(cluster_id) == TRUE]
missing_editors <- missing_editors[, .(pair_id, source_id, source_title, editor)]
missing_editors <- missing_editors[!duplicated(missing_editors)]
fwrite(
missing_editors,
'../../data_collection/3_editor_clusterids/followup_manual_matching/no_initial_match.csv'
)
# Reduce paper dataset to those that belong to an author with the same
# last name as one of the missing editors
get_last_name <- function(full_name){
temp_names <- strsplit(full_name, ' ')
temp_last_name <- temp_names[[1]][length(temp_names[[1]])]
return(tolower(temp_last_name))
}
missing_editors$last_name <- unlist(lapply(missing_editors$editor, get_last_name))
possible_authors <- open_dataset('../../data/author_details') %>%
filter(last_name %in% missing_editors$last_name) %>%
collect()
possible_papers <- open_dataset('../../data/paper_authors_affiliations') %>%
filter(cluster_id %in% possible_authors$cluster_id) %>%
select(ut, cluster_id) %>%
collect()
possible_papers <- left_join(
possible_papers,
open_dataset('../../../sci_elites_wos/data/processed/paper_details') %>%
filter(ut %in% possible_papers$ut) %>%
select(ut, doi) %>%
collect(),
by='ut'
)
possible_papers <- left_join(
possible_papers,
possible_authors,
by='cluster_id'
)
setorder(possible_papers, full_name, cluster_id)
fwrite(
possible_papers,
'../../data_collection/3_editor_clusterids/followup_manual_matching/no_initial_match_papers.csv'
)
editors
editors[source_id == 1396]
editors
editors[is.na(cluster_id) == TRUE]
test <-editors[is.na(cluster_id) == TRUE]
test[, length(unique(editor))]
test[source_id == 1396]
rbind(
editors[is.na(cluster_id) == TRUE],
editors_partial[is.na(cluster_id) == TRUE]
)
rbind(
editors[is.na(cluster_id) == TRUE],
editors_partial[is.na(cluster_id) == TRUE],
fill=TRUE
)
missing_editors <- rbind(
editors[is.na(cluster_id) == TRUE],
editors_partial[is.na(cluster_id) == TRUE],
fill=TRUE
)
missing_editors <- missing_editors[, .(pair_id, source_id, source_title, editor)]
missing_editors <- missing_editors[!duplicated(missing_editors)]
missing_editors
missing_editors
fwrite(
missing_editors,
'../../data_collection/3_editor_clusterids/followup_manual_matching/no_initial_match.csv'
)
missing_editors
# Reduce paper dataset to those that belong to an author with the same
# last name as one of the missing editors
get_last_name <- function(full_name){
temp_names <- strsplit(full_name, ' ')
temp_last_name <- temp_names[[1]][length(temp_names[[1]])]
return(tolower(temp_last_name))
}
missing_editors$last_name <- unlist(lapply(missing_editors$editor, get_last_name))
possible_authors <- open_dataset('../../data/author_details') %>%
filter(last_name %in% missing_editors$last_name) %>%
collect()
possible_papers <- open_dataset('../../data/paper_authors_affiliations') %>%
filter(cluster_id %in% possible_authors$cluster_id) %>%
select(ut, cluster_id) %>%
collect()
possible_papers <- left_join(
possible_papers,
open_dataset('../../../sci_elites_wos/data/processed/paper_details') %>%
filter(ut %in% possible_papers$ut) %>%
select(ut, doi) %>%
collect(),
by='ut'
)
possible_papers <- left_join(
possible_papers,
possible_authors,
by='cluster_id'
)
setorder(possible_papers, full_name, cluster_id)
fwrite(
possible_papers,
'../../data_collection/3_editor_clusterids/followup_manual_matching/no_initial_match_papers.csv'
)
possible_papers
missing_editors
library(arrow, quietly=TRUE, warn.conflicts=FALSE)
library(data.table, quietly=TRUE, warn.conflicts=FALSE)
library(dplyr, quietly=TRUE, warn.conflicts=FALSE)
library(tidyr, quietly=TRUE, warn.conflicts=FALSE)
transitions <- fread('../../data/editorial_transitions/editorial_transitions.csv')
editors <- merge(
fread('../../data/editorial_transitions/editors_beforeafter.csv'),
fread('../../data_collection/3_editor_clusterids/editor_clusterids.csv', select=c('pair_id', 'cluster_id')),
by='pair_id',
all.x=TRUE
)
editors_partial <- merge(
fread('../../data/editorial_transitions/editors_partialtransitions_beforeafter.csv'),
fread('../../data_collection/3_editor_clusterids/editor_clusterids.csv', select=c('pair_id', 'cluster_id')),
by='pair_id',
all.x=TRUE
)
missing_editors <- rbind(
editors[is.na(cluster_id) == TRUE],
editors_partial[is.na(cluster_id) == TRUE],
fill=TRUE
)
missing_editors <- missing_editors[, .(pair_id, source_id, source_title, editor)]
missing_editors <- missing_editors[!duplicated(missing_editors)]
fwrite(
missing_editors,
'../../data_collection/3_editor_clusterids/followup_manual_matching/no_initial_match.csv'
)
missing_editors
# Reduce paper dataset to those that belong to an author with the same
# last name as one of the missing editors
get_last_name <- function(full_name){
temp_names <- strsplit(full_name, ' ')
temp_last_name <- temp_names[[1]][length(temp_names[[1]])]
return(tolower(temp_last_name))
}
missing_editors$last_name <- unlist(lapply(missing_editors$editor, get_last_name))
possible_authors <- open_dataset('../../data/author_details') %>%
filter(last_name %in% missing_editors$last_name) %>%
collect()
possible_papers <- open_dataset('../../data/paper_authors_affiliations') %>%
filter(cluster_id %in% possible_authors$cluster_id) %>%
select(ut, cluster_id) %>%
collect()
possible_papers <- left_join(
possible_papers,
open_dataset('../../../sci_elites_wos/data/processed/paper_details') %>%
filter(ut %in% possible_papers$ut) %>%
select(ut, doi) %>%
collect(),
by='ut'
)
possible_papers <- left_join(
possible_papers,
possible_authors,
by='cluster_id'
)
setorder(possible_papers, full_name, cluster_id)
fwrite(
possible_papers,
'../../data_collection/3_editor_clusterids/followup_manual_matching/no_initial_match_papers.csv'
)
possible_papers
possible_papers[, .(cluster_id, first_name, last_name, full_name, doi)]
fwrite(
possible_papers[, .(cluster_id, first_name, last_name, full_name, doi)],
'../../data_collection/3_editor_clusterids/followup_manual_matching/no_initial_match_papers.csv'
)
setorder(possible_papers, full_name, last_name, first_name, cluster_id)
possible_papers
possible_papers[doi !- '']
possible_papers[doi != '']
fwrite(
possible_papers[doi != '', .(cluster_id, first_name, last_name, full_name, doi)],
'../../data_collection/3_editor_clusterids/followup_manual_matching/no_initial_match_papers.csv'
)
library(data.table, warn.conflicts = F, quietly = T)
library(dplyr, warn.conflicts = F, quietly = T)
library(tidyr, warn.conflicts = F, quietly = T)
library(arrow, warn.conflicts = F, quietly = T)
import_data <- function(level, window_size, relevant_scientists){
if(level == 'countries'){
return(
as.data.table(
open_dataset('../../data_migration/author_countries_yearly') %>%
filter(cluster_id %in% relevant_scientists,
window_plusminus_n == window_size) %>%
collect()
)
)
}else if(level == 'subregions'){
return(
as.data.table(
open_dataset('../../data_migration/author_subregions_yearly') %>%
filter(cluster_id %in% relevant_scientists,
window_plusminus_n == window_size) %>%
collect()
)
)
}else{
return(
as.data.table(
open_dataset('../../data_migration/author_northsouth_yearly') %>%
filter(cluster_id %in% relevant_scientists,
window_plusminus_n == window_size) %>%
collect()
)
)
}
}
levels_of_analysis <- c('countries', 'subregions', 'northsouth')
geo_vars <- c('country_iso_alpha3_code', 'un_subregion', 'global_north')
window_sizes <- 1:2
cohorts_of_interest <- 2008:2020
relevant_years <- 2008:2022
relevant_scientists <- open_dataset('../../data_migration/author_details') %>%
filter(first_year %in% 2008:2020) %>%
select(cluster_id) %>%
collect()
relevant_scientists <- relevant_scientists$cluster_id
level <- levels_of_analysis[1]
window_size <- window_sizes[1]
aff_data <- import_data(level, window_size, relevant_scientists)
aff_data[cluster_id == 7206791]
# Assemble start/end years for affiliations
setorder(aff_data, cluster_id, eval_year)
aff_data[
,
'sequence_id' := rleid(
c(TRUE, diff(eval_year) == 1)
),
by=c('cluster_id', geo_var)
]
geo_var <- geo_vars[1]
aff_data[
,
'sequence_id' := rleid(
c(TRUE, diff(eval_year) == 1)
),
by=c('cluster_id', geo_var)
]
# There are weird exceptions to this process. The first year of a new transition
# sequence gets counted separately from the rest of the run. So it needs to be
# re-joined with the rest.
# Add one to run ids that are even, so the exception rows discussed above
# get included with the rest of the sequence. Ceiling and /2 returns the
# sequence_id to a sequence of adjacent numbers (e.g. 1, 2, 3 instead of
# 1, 3, 5)
aff_data[sequence_id %% 2 == 0, sequence_id := sequence_id + 1]
aff_data[, sequence_id := ceiling(sequence_id/2)]
library(devtools)
devtools::install_github('klutometis/roxygen')
library(roxygen2)
aff_data
aff_data[cluster_id == 10]
z <- aff_data[cluster_id == 10]
z
z[, diff(eval_year)]
z$eval_year[-1L]
z$eval_year[-9] + 1L
z$eval_year
z$eval_year[-9]
z$eval_year[-1] != z$eval_year[-9] + 1
y <- z$eval_year[-1] != z$eval_year[-9] + 1
which(y|is.na(y))
c(which(y|is.na(y)), 9)
i <- c(which(y|is.na(y)), 9)
i
diff(c(0, i))
z$eval_year[, -1]
z$eval_year[head(c(0, i) + 1, -1]
z$eval_year[head(c(0, i) + 1, -1)]
y
seq_along
i
diff(c(0, i))
temp_data[, .N, by=c('cluster_id', geo_var)]
temp_aff[, .N, by=c('cluster_id', geo_var)]
aff_data[, .N, by=c('cluster_id', geo_var)]
aff_data[, 'n_bygeo' := .N, by=c('cluster_id', geo_var)]
y
i
counter <- 1
for(switch in i){
counter + 1
}
for(switch in i){
counter <- counter + 1
}
counter <- 1
s <- list()
for(switch in i){
append(s, rep(counter, switch))
counter <- counter + 1
}
help(append)
for(switch in i){
s <- append(s, rep(counter, switch))
counter <- counter + 1
}
counter <- 1
s <- list()
for(switch in i){
s <- append(s, rep(counter, switch))
counter <- counter + 1
}
s
s <- c()
s <- vector()
counter <- 1
for(switch in i){
s <- append(s, rep(counter, switch))
counter <- counter + 1
}
s
# Adapted from https://stackoverflow.com/questions/7077710/sequence-length-encoding-using-r
getSequences <- function(year_sequence){
if(!is.numeric(year_sequence)){
year_sequence <- as.numeric(year_sequence)
}
n <- length(year_sequence)
boolean_sequence <- year_sequence[-1L] != year_sequence[-n] + 1L
switches <- c(which(boolean_sequence|is.na(boolean_sequence)),n)
counter <- 1
sequence_vector <- vector()
for(switch in switches){
sequence_vector <- append(sequence_vector, rep(counter, switch))
counter <- counter + 1
}
return(sequence_vector)
}
getSequences(z$eval_year)
aff_data <- import_data(level, window_size, relevant_scientists)
aff_data[, sequence_id := getSequences(year_sequence)]
aff_data[, sequence_id := getSequences(eval_year)]
aff_data[, sequence_id := getSequences(eval_year), by=c('cluster_id', geo_var)]
aff_data
aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG']
aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG', eval_year]
aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG', getSequences(eval_year)]
# Assemble start/end years for affiliations
setorder(aff_data, cluster_id, eval_year)
aff_data[, sequence_id := getSequences(eval_year), by=c('cluster_id', geo_var)]
aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG', getSequences(eval_year)]
length(aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG', getSequences(eval_year)])
length(aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG'])
aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG']
help(rep)
rep(1, times=1)
# Adapted from https://stackoverflow.com/questions/7077710/sequence-length-encoding-using-r
getSequences <- function(year_sequence){
if(!is.numeric(year_sequence)){
year_sequence <- as.numeric(year_sequence)
}
n <- length(year_sequence)
boolean_sequence <- year_sequence[-1L] != year_sequence[-n] + 1L
switches <- c(which(boolean_sequence|is.na(boolean_sequence)),n)
counter <- 1
sequence_vector <- vector()
for(switch in switches){
sequence_vector <- append(sequence_vector, rep(counter, times=switch))
counter <- counter + 1
}
return(sequence_vector)
}
aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG', getSequences(eval_year)]
# Adapted from https://stackoverflow.com/questions/7077710/sequence-length-encoding-using-r
getSequences <- function(year_sequence){
if(!is.numeric(year_sequence)){
year_sequence <- as.numeric(year_sequence)
}
n <- length(year_sequence)
boolean_sequence <- year_sequence[-1L] != year_sequence[-n] + 1L
switches <- c(which(boolean_sequence|is.na(boolean_sequence)),n)
switch_lengths <- diff(c(0L,switches))
counter <- 1
sequence_vector <- vector()
for(switch_length in switch_lengths){
sequence_vector <- append(sequence_vector, rep(counter, times=switch_length))
counter <- counter + 1
}
return(sequence_vector)
}
aff_data[cluster_id == 7206791 & country_iso_alpha3_code == 'ARG', getSequences(eval_year)]
aff_data[, sequence_id := getSequences(eval_year), by=c('cluster_id', geo_var)]
aff_data
aff_data[sequence_id > 1]
aff_data[sequence_id > 2]
aff_data[cluster_id == 1176]
# Exclude non-migrants
aff_data <- aff_data[
cluster_id %in%
aff_data[, .N, by=c('cluster_id', geo_var)][, .N, by='cluster_id'][N > 1, cluster_id]
]
aff_data
aff_data[sequence_id > 2]
aff_data[cluster_id == 94806059]
aff_data <- aff_data[
,
list(
'min_year' = min(eval_year),
'max_year' = max(eval_year)
),
by=c('cluster_id', geo_var, 'sequence_id')
]
aff_data[cluster_id == 94806059]
aff_data <- import_data(level, window_size, relevant_scientists)
source('biblio_migration/getAffiliationTenures.R')
tenure_data <- getAffiliationTenures(aff_data, geo_var, ignore_nonmigrant_gaps = TRUE)
tenure_data
aff_data[cluster_id == 93600478]
# Assemble start/end years for affiliations
setorder(aff_data, cluster_id, eval_year)
source('biblio_migration/getAffiliationTenures.R')
tenure_data <- getAffiliationTenures(aff_data, geo_var, ignore_nonmigrant_gaps = TRUE)
tenure_data
tenure_data[sequence_id > 2]
source('biblio_migration/getAffiliationTenures.R')
vignette('rd')
vignette('roxygen2')
library(roxygen2)
vignette('roxygen2')
source('biblio_migration/getAffiliationTenures.R')
tenure_data <- getAffiliationTenures(aff_data, geo_var, ignore_nonmigrant_gaps = TRUE)
colnames(aff_data)
colnames(aff_data)[colnames(aff_data) == 'cluster_id'] <- 'person_id'
source('biblio_migration/getAffiliationTenures.R')
colnames(aff_data)[colnames(aff_data) == 'cluster_id'] <- 'person_id'
tenure_data <- getAffiliationTenures(
aff_data,
'person_id',
'eval_year',
geo_var,
ignore_nonmigrant_gaps = TRUE)
tenure_data <- getAffiliationTenures(
aff_data,
'person_id',
'eval_year',
geo_var,
ignore_nonmobile_gaps = TRUE)
source('biblio_migration/getAffiliationTenures.R')
tenure_data <- getAffiliationTenures(
aff_data,
'person_id',
'eval_year',
geo_var,
ignore_nonmobile_gaps = TRUE)
source('biblio_migration/getAffiliationTenures.R')
tenure_data <- getAffiliationTenures(
aff_data,
'person_id',
'eval_year',
geo_var,
ignore_nonmobile_gaps = TRUE)
source('biblio_migration/getAffiliationTenures.R')
tenure_data <- getAffiliationTenures(
aff_data,
'person_id',
'eval_year',
geo_var,
ignore_nonmobile_gaps = TRUE)
setwd('biblio_migration')
setwd('biblio_migration')
document()
document()
getwd()
setwd('Dropbox/research/in_progress/north_south_elites/')
setwd('code_migration/1_affiliation_events')
setwd('biblio_migration')
document()
help("document")
create('biblio_migration')
setwd('..')
getwd()
ls()
setwd('hsw396')
setwd('Dropbox/research/in_progress/north_south_elites/')
setwd('code_migration/1_affiliation_events')
create('biblio_migration')
create('biblio.migration')
setwd('./biblio.migration')
document()
getwd()
document()
